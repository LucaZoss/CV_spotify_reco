{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras_preprocessing\n",
      "  Obtaining dependency information for keras_preprocessing from https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /Users/constantinwiederin/anaconda3/lib/python3.11/site-packages (from keras_preprocessing) (1.25.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/constantinwiederin/anaconda3/lib/python3.11/site-packages (from keras_preprocessing) (1.16.0)\n",
      "Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m253.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m--:--\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: keras_preprocessing\n",
      "Successfully installed keras_preprocessing-1.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install keras_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.image import load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = '/Users/constantinwiederin/Documents/IE/Computer vision/images 2/train'\n",
    "TEST_DIR = '/Users/constantinwiederin/Documents/IE/Computer vision/images 2/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy completed\n",
      "sad completed\n",
      "fear completed\n",
      "surprise completed\n",
      "neutral completed\n",
      "angry completed\n",
      "disgust completed\n",
      "                                                   image    label\n",
      "0      /Users/constantinwiederin/Documents/IE/Compute...    happy\n",
      "1      /Users/constantinwiederin/Documents/IE/Compute...    happy\n",
      "2      /Users/constantinwiederin/Documents/IE/Compute...    happy\n",
      "3      /Users/constantinwiederin/Documents/IE/Compute...    happy\n",
      "4      /Users/constantinwiederin/Documents/IE/Compute...    happy\n",
      "...                                                  ...      ...\n",
      "28816  /Users/constantinwiederin/Documents/IE/Compute...  disgust\n",
      "28817  /Users/constantinwiederin/Documents/IE/Compute...  disgust\n",
      "28818  /Users/constantinwiederin/Documents/IE/Compute...  disgust\n",
      "28819  /Users/constantinwiederin/Documents/IE/Compute...  disgust\n",
      "28820  /Users/constantinwiederin/Documents/IE/Compute...  disgust\n",
      "\n",
      "[28821 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def createdataframe(dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for label in os.listdir(dir):\n",
    "        if label == '.DS_Store':\n",
    "            continue\n",
    "        for imagename in os.listdir(os.path.join(dir, label)):\n",
    "            image_paths.append(os.path.join(dir, label, imagename))\n",
    "            labels.append(label)\n",
    "        print(label, \"completed\")\n",
    "    return image_paths, labels\n",
    "\n",
    "train = pd.DataFrame()\n",
    "train['image'], train['label'] = createdataframe(TRAIN_DIR)\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy completed\n",
      "sad completed\n",
      "fear completed\n",
      "surprise completed\n",
      "neutral completed\n",
      "angry completed\n",
      "disgust completed\n",
      "                                                  image    label\n",
      "0     /Users/constantinwiederin/Documents/IE/Compute...    happy\n",
      "1     /Users/constantinwiederin/Documents/IE/Compute...    happy\n",
      "2     /Users/constantinwiederin/Documents/IE/Compute...    happy\n",
      "3     /Users/constantinwiederin/Documents/IE/Compute...    happy\n",
      "4     /Users/constantinwiederin/Documents/IE/Compute...    happy\n",
      "...                                                 ...      ...\n",
      "7061  /Users/constantinwiederin/Documents/IE/Compute...  disgust\n",
      "7062  /Users/constantinwiederin/Documents/IE/Compute...  disgust\n",
      "7063  /Users/constantinwiederin/Documents/IE/Compute...  disgust\n",
      "7064  /Users/constantinwiederin/Documents/IE/Compute...  disgust\n",
      "7065  /Users/constantinwiederin/Documents/IE/Compute...  disgust\n",
      "\n",
      "[7066 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "test = pd.DataFrame()\n",
    "test['image'], test['label'] = createdataframe(TEST_DIR)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a64bf96c544366ba959d3cac5f40f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28821 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/constantinwiederin/anaconda3/lib/python3.11/site-packages/keras_preprocessing/image/utils.py:107: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c73a8b7997495f81a5a49b13acd3a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7066 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_features(images):\n",
    "    features = []\n",
    "    for image in tqdm(images):\n",
    "        img = load_img(image, grayscale=True)\n",
    "        img = np.array(img)\n",
    "        features.append(img)\n",
    "    features = np.array(features)\n",
    "    features = features.reshape(len(features), 48, 48, 1)\n",
    "    return features\n",
    "\n",
    "train_features = extract_features(train['image'])\n",
    "test_features = extract_features(test['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_features / 255.0\n",
    "x_test = test_features / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(train['label'])\n",
    "y_train = le.transform(train['label'])\n",
    "y_test = le.transform(test['label'])\n",
    "y_train = to_categorical(y_train, num_classes=7)\n",
    "y_test = to_categorical(y_test, num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# convolutional layers\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation='relu', input_shape=(48,48,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# flattening\n",
    "model.add(Flatten())\n",
    "\n",
    "# fully connected layers\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "# model compilation\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 867ms/step - accuracy: 0.2343 - loss: 1.8383 - val_accuracy: 0.2583 - val_loss: 1.8038\n",
      "Epoch 2/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 842ms/step - accuracy: 0.2501 - loss: 1.8034 - val_accuracy: 0.2819 - val_loss: 1.7335\n",
      "Epoch 3/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 967ms/step - accuracy: 0.2830 - loss: 1.7341 - val_accuracy: 0.3596 - val_loss: 1.6156\n",
      "Epoch 4/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 1s/step - accuracy: 0.3545 - loss: 1.6272 - val_accuracy: 0.4254 - val_loss: 1.4763\n",
      "Epoch 5/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 1s/step - accuracy: 0.3980 - loss: 1.5252 - val_accuracy: 0.4611 - val_loss: 1.3841\n",
      "Epoch 6/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 1s/step - accuracy: 0.4272 - loss: 1.4719 - val_accuracy: 0.4810 - val_loss: 1.3629\n",
      "Epoch 7/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 994ms/step - accuracy: 0.4488 - loss: 1.4219 - val_accuracy: 0.4948 - val_loss: 1.3022\n",
      "Epoch 8/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 1s/step - accuracy: 0.4684 - loss: 1.3738 - val_accuracy: 0.5154 - val_loss: 1.2674\n",
      "Epoch 9/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 1s/step - accuracy: 0.4806 - loss: 1.3496 - val_accuracy: 0.5224 - val_loss: 1.2481\n",
      "Epoch 10/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 1s/step - accuracy: 0.4908 - loss: 1.3238 - val_accuracy: 0.5304 - val_loss: 1.2249\n",
      "Epoch 11/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 1s/step - accuracy: 0.5007 - loss: 1.3066 - val_accuracy: 0.5359 - val_loss: 1.2072\n",
      "Epoch 12/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 1s/step - accuracy: 0.5097 - loss: 1.2817 - val_accuracy: 0.5423 - val_loss: 1.2001\n",
      "Epoch 13/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 1s/step - accuracy: 0.5127 - loss: 1.2770 - val_accuracy: 0.5459 - val_loss: 1.1829\n",
      "Epoch 14/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 1s/step - accuracy: 0.5179 - loss: 1.2543 - val_accuracy: 0.5534 - val_loss: 1.1765\n",
      "Epoch 15/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 1s/step - accuracy: 0.5290 - loss: 1.2362 - val_accuracy: 0.5577 - val_loss: 1.1564\n",
      "Epoch 16/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 1s/step - accuracy: 0.5304 - loss: 1.2327 - val_accuracy: 0.5607 - val_loss: 1.1452\n",
      "Epoch 17/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 1s/step - accuracy: 0.5433 - loss: 1.2087 - val_accuracy: 0.5641 - val_loss: 1.1384\n",
      "Epoch 18/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m279s\u001b[0m 1s/step - accuracy: 0.5432 - loss: 1.1977 - val_accuracy: 0.5676 - val_loss: 1.1314\n",
      "Epoch 19/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 1s/step - accuracy: 0.5411 - loss: 1.2003 - val_accuracy: 0.5662 - val_loss: 1.1237\n",
      "Epoch 20/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 1s/step - accuracy: 0.5510 - loss: 1.1721 - val_accuracy: 0.5784 - val_loss: 1.1071\n",
      "Epoch 21/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1443s\u001b[0m 6s/step - accuracy: 0.5623 - loss: 1.1573 - val_accuracy: 0.5793 - val_loss: 1.1015\n",
      "Epoch 22/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1327s\u001b[0m 6s/step - accuracy: 0.5538 - loss: 1.1687 - val_accuracy: 0.5794 - val_loss: 1.1072\n",
      "Epoch 23/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 942ms/step - accuracy: 0.5625 - loss: 1.1493 - val_accuracy: 0.5768 - val_loss: 1.1008\n",
      "Epoch 24/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 942ms/step - accuracy: 0.5699 - loss: 1.1345 - val_accuracy: 0.5897 - val_loss: 1.0926\n",
      "Epoch 25/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 961ms/step - accuracy: 0.5693 - loss: 1.1276 - val_accuracy: 0.5868 - val_loss: 1.0871\n",
      "Epoch 26/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 966ms/step - accuracy: 0.5703 - loss: 1.1258 - val_accuracy: 0.5882 - val_loss: 1.0896\n",
      "Epoch 27/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 1s/step - accuracy: 0.5845 - loss: 1.1017 - val_accuracy: 0.5940 - val_loss: 1.0849\n",
      "Epoch 28/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m248s\u001b[0m 1s/step - accuracy: 0.5868 - loss: 1.0919 - val_accuracy: 0.5964 - val_loss: 1.0799\n",
      "Epoch 29/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 1s/step - accuracy: 0.5857 - loss: 1.0971 - val_accuracy: 0.5916 - val_loss: 1.0789\n",
      "Epoch 30/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 1s/step - accuracy: 0.5820 - loss: 1.1050 - val_accuracy: 0.5974 - val_loss: 1.0645\n",
      "Epoch 31/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 1s/step - accuracy: 0.5864 - loss: 1.0983 - val_accuracy: 0.5947 - val_loss: 1.0749\n",
      "Epoch 32/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 1s/step - accuracy: 0.5931 - loss: 1.0829 - val_accuracy: 0.5958 - val_loss: 1.0691\n",
      "Epoch 33/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 1s/step - accuracy: 0.5989 - loss: 1.0700 - val_accuracy: 0.5903 - val_loss: 1.0750\n",
      "Epoch 34/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 2s/step - accuracy: 0.5957 - loss: 1.0682 - val_accuracy: 0.6006 - val_loss: 1.0571\n",
      "Epoch 35/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m427s\u001b[0m 2s/step - accuracy: 0.6025 - loss: 1.0662 - val_accuracy: 0.6010 - val_loss: 1.0622\n",
      "Epoch 36/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m419s\u001b[0m 2s/step - accuracy: 0.6040 - loss: 1.0517 - val_accuracy: 0.6066 - val_loss: 1.0620\n",
      "Epoch 37/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 2s/step - accuracy: 0.6070 - loss: 1.0444 - val_accuracy: 0.6001 - val_loss: 1.0660\n",
      "Epoch 38/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 1s/step - accuracy: 0.6096 - loss: 1.0379 - val_accuracy: 0.6102 - val_loss: 1.0457\n",
      "Epoch 39/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 1s/step - accuracy: 0.6122 - loss: 1.0201 - val_accuracy: 0.6005 - val_loss: 1.0552\n",
      "Epoch 40/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m310s\u001b[0m 1s/step - accuracy: 0.6157 - loss: 1.0217 - val_accuracy: 0.6067 - val_loss: 1.0485\n",
      "Epoch 41/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 1s/step - accuracy: 0.6094 - loss: 1.0243 - val_accuracy: 0.6073 - val_loss: 1.0450\n",
      "Epoch 42/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 1s/step - accuracy: 0.6169 - loss: 1.0152 - val_accuracy: 0.6102 - val_loss: 1.0436\n",
      "Epoch 43/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 1s/step - accuracy: 0.6191 - loss: 1.0037 - val_accuracy: 0.6180 - val_loss: 1.0312\n",
      "Epoch 44/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 1s/step - accuracy: 0.6232 - loss: 1.0039 - val_accuracy: 0.6110 - val_loss: 1.0409\n",
      "Epoch 45/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 1s/step - accuracy: 0.6191 - loss: 1.0058 - val_accuracy: 0.6176 - val_loss: 1.0414\n",
      "Epoch 46/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 1s/step - accuracy: 0.6233 - loss: 1.0030 - val_accuracy: 0.6095 - val_loss: 1.0456\n",
      "Epoch 47/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 1s/step - accuracy: 0.6272 - loss: 0.9918 - val_accuracy: 0.6110 - val_loss: 1.0373\n",
      "Epoch 48/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 1s/step - accuracy: 0.6345 - loss: 0.9855 - val_accuracy: 0.6163 - val_loss: 1.0296\n",
      "Epoch 49/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 947ms/step - accuracy: 0.6294 - loss: 0.9804 - val_accuracy: 0.6152 - val_loss: 1.0345\n",
      "Epoch 50/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 990ms/step - accuracy: 0.6314 - loss: 0.9765 - val_accuracy: 0.6124 - val_loss: 1.0379\n",
      "Epoch 51/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 958ms/step - accuracy: 0.6272 - loss: 0.9798 - val_accuracy: 0.6111 - val_loss: 1.0441\n",
      "Epoch 52/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 943ms/step - accuracy: 0.6407 - loss: 0.9653 - val_accuracy: 0.6173 - val_loss: 1.0368\n",
      "Epoch 53/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 892ms/step - accuracy: 0.6386 - loss: 0.9754 - val_accuracy: 0.6160 - val_loss: 1.0260\n",
      "Epoch 54/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 886ms/step - accuracy: 0.6352 - loss: 0.9703 - val_accuracy: 0.6146 - val_loss: 1.0348\n",
      "Epoch 55/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 918ms/step - accuracy: 0.6451 - loss: 0.9474 - val_accuracy: 0.6131 - val_loss: 1.0260\n",
      "Epoch 56/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 930ms/step - accuracy: 0.6392 - loss: 0.9517 - val_accuracy: 0.6207 - val_loss: 1.0305\n",
      "Epoch 57/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 1s/step - accuracy: 0.6434 - loss: 0.9445 - val_accuracy: 0.6192 - val_loss: 1.0341\n",
      "Epoch 58/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 963ms/step - accuracy: 0.6517 - loss: 0.9357 - val_accuracy: 0.6102 - val_loss: 1.0444\n",
      "Epoch 59/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 998ms/step - accuracy: 0.6516 - loss: 0.9347 - val_accuracy: 0.6125 - val_loss: 1.0440\n",
      "Epoch 60/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 987ms/step - accuracy: 0.6537 - loss: 0.9250 - val_accuracy: 0.6193 - val_loss: 1.0391\n",
      "Epoch 61/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 926ms/step - accuracy: 0.6550 - loss: 0.9181 - val_accuracy: 0.6235 - val_loss: 1.0220\n",
      "Epoch 62/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 858ms/step - accuracy: 0.6592 - loss: 0.9197 - val_accuracy: 0.6189 - val_loss: 1.0367\n",
      "Epoch 63/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 903ms/step - accuracy: 0.6572 - loss: 0.9137 - val_accuracy: 0.6204 - val_loss: 1.0357\n",
      "Epoch 64/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 1s/step - accuracy: 0.6621 - loss: 0.9045 - val_accuracy: 0.6223 - val_loss: 1.0291\n",
      "Epoch 65/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 984ms/step - accuracy: 0.6639 - loss: 0.8989 - val_accuracy: 0.6244 - val_loss: 1.0365\n",
      "Epoch 66/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 1s/step - accuracy: 0.6646 - loss: 0.9002 - val_accuracy: 0.6313 - val_loss: 1.0217\n",
      "Epoch 67/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 953ms/step - accuracy: 0.6618 - loss: 0.9077 - val_accuracy: 0.6269 - val_loss: 1.0304\n",
      "Epoch 68/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 961ms/step - accuracy: 0.6660 - loss: 0.8967 - val_accuracy: 0.6235 - val_loss: 1.0311\n",
      "Epoch 69/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 974ms/step - accuracy: 0.6712 - loss: 0.8836 - val_accuracy: 0.6219 - val_loss: 1.0376\n",
      "Epoch 70/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 927ms/step - accuracy: 0.6670 - loss: 0.8950 - val_accuracy: 0.6224 - val_loss: 1.0329\n",
      "Epoch 71/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 863ms/step - accuracy: 0.6729 - loss: 0.8832 - val_accuracy: 0.6173 - val_loss: 1.0371\n",
      "Epoch 72/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 949ms/step - accuracy: 0.6711 - loss: 0.8836 - val_accuracy: 0.6274 - val_loss: 1.0245\n",
      "Epoch 73/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 930ms/step - accuracy: 0.6787 - loss: 0.8632 - val_accuracy: 0.6313 - val_loss: 1.0184\n",
      "Epoch 74/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 994ms/step - accuracy: 0.6860 - loss: 0.8553 - val_accuracy: 0.6226 - val_loss: 1.0226\n",
      "Epoch 75/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 1s/step - accuracy: 0.6775 - loss: 0.8650 - val_accuracy: 0.6286 - val_loss: 1.0235\n",
      "Epoch 76/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 1s/step - accuracy: 0.6774 - loss: 0.8649 - val_accuracy: 0.6255 - val_loss: 1.0291\n",
      "Epoch 77/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 1s/step - accuracy: 0.6787 - loss: 0.8616 - val_accuracy: 0.6291 - val_loss: 1.0357\n",
      "Epoch 78/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m248s\u001b[0m 1s/step - accuracy: 0.6894 - loss: 0.8493 - val_accuracy: 0.6267 - val_loss: 1.0299\n",
      "Epoch 79/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 1s/step - accuracy: 0.6830 - loss: 0.8484 - val_accuracy: 0.6265 - val_loss: 1.0208\n",
      "Epoch 80/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 1s/step - accuracy: 0.6871 - loss: 0.8351 - val_accuracy: 0.6234 - val_loss: 1.0327\n",
      "Epoch 81/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 1s/step - accuracy: 0.6870 - loss: 0.8383 - val_accuracy: 0.6272 - val_loss: 1.0226\n",
      "Epoch 82/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 1s/step - accuracy: 0.6940 - loss: 0.8322 - val_accuracy: 0.6245 - val_loss: 1.0300\n",
      "Epoch 83/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 1s/step - accuracy: 0.6883 - loss: 0.8346 - val_accuracy: 0.6235 - val_loss: 1.0281\n",
      "Epoch 84/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 1s/step - accuracy: 0.6900 - loss: 0.8428 - val_accuracy: 0.6231 - val_loss: 1.0335\n",
      "Epoch 85/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 1s/step - accuracy: 0.6943 - loss: 0.8298 - val_accuracy: 0.6295 - val_loss: 1.0278\n",
      "Epoch 86/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 1s/step - accuracy: 0.6948 - loss: 0.8243 - val_accuracy: 0.6265 - val_loss: 1.0278\n",
      "Epoch 87/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 1s/step - accuracy: 0.6977 - loss: 0.8250 - val_accuracy: 0.6296 - val_loss: 1.0271\n",
      "Epoch 88/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 1s/step - accuracy: 0.7030 - loss: 0.8157 - val_accuracy: 0.6245 - val_loss: 1.0283\n",
      "Epoch 89/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 1s/step - accuracy: 0.6980 - loss: 0.8178 - val_accuracy: 0.6286 - val_loss: 1.0410\n",
      "Epoch 90/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m281s\u001b[0m 1s/step - accuracy: 0.6982 - loss: 0.8070 - val_accuracy: 0.6286 - val_loss: 1.0315\n",
      "Epoch 91/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m285s\u001b[0m 1s/step - accuracy: 0.6997 - loss: 0.8088 - val_accuracy: 0.6269 - val_loss: 1.0307\n",
      "Epoch 92/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 1s/step - accuracy: 0.7052 - loss: 0.7983 - val_accuracy: 0.6279 - val_loss: 1.0348\n",
      "Epoch 93/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 1s/step - accuracy: 0.7069 - loss: 0.7925 - val_accuracy: 0.6272 - val_loss: 1.0338\n",
      "Epoch 94/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 1s/step - accuracy: 0.7061 - loss: 0.8019 - val_accuracy: 0.6294 - val_loss: 1.0436\n",
      "Epoch 95/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 1s/step - accuracy: 0.7042 - loss: 0.8024 - val_accuracy: 0.6274 - val_loss: 1.0411\n",
      "Epoch 96/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 1s/step - accuracy: 0.7116 - loss: 0.7878 - val_accuracy: 0.6234 - val_loss: 1.0475\n",
      "Epoch 97/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 1s/step - accuracy: 0.7101 - loss: 0.7897 - val_accuracy: 0.6289 - val_loss: 1.0370\n",
      "Epoch 98/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 1s/step - accuracy: 0.7118 - loss: 0.7831 - val_accuracy: 0.6285 - val_loss: 1.0450\n",
      "Epoch 99/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 1s/step - accuracy: 0.7134 - loss: 0.7778 - val_accuracy: 0.6284 - val_loss: 1.0333\n",
      "Epoch 100/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 1s/step - accuracy: 0.7212 - loss: 0.7647 - val_accuracy: 0.6325 - val_loss: 1.0369\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x31ee50dd0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_train, y=y_train, batch_size=128, epochs=100, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"emotiondetector.json\", 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save(\"emotiondetector.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/constantinwiederin/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "json_file = open(\"emotiondetector.json\", \"r\")\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights(\"emotiondetector.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "def ef(image):\n",
    "    img = load_img(image, grayscale=True)\n",
    "    feature = np.array(img)\n",
    "    feature = feature.reshape(1, 48, 48, 1)\n",
    "    return feature / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image is of sad\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "model prediction is  surprise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/constantinwiederin/anaconda3/lib/python3.11/site-packages/keras_preprocessing/image/utils.py:107: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    }
   ],
   "source": [
    "image = '/Users/constantinwiederin/Documents/IE/Computer vision/images 2/images/train/surprise/55.jpg'\n",
    "print(\"original image is of sad\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \", pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "image = 'images/train/sad/42.jpg'\n",
    "print(\"original image is of sad\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \", pred_label)\n",
    "plt.imshow(img.reshape(48, 48), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'facialemotionmodel.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 17\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X24sdW50aXRsZWQ%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X24sdW50aXRsZWQ%3D?line=4'>5</a>\u001b[0m \u001b[39m# Load the pre-trained model architecture from JSON file\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X24sdW50aXRsZWQ%3D?line=5'>6</a>\u001b[0m json_file \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mfacialemotionmodel.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X24sdW50aXRsZWQ%3D?line=6'>7</a>\u001b[0m model_json \u001b[39m=\u001b[39m json_file\u001b[39m.\u001b[39mread()\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X24sdW50aXRsZWQ%3D?line=7'>8</a>\u001b[0m json_file\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'facialemotionmodel.json'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from keras.models import model_from_json\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained model architecture from JSON file\n",
    "json_file = open(\"facialemotionmodel.json\", \"r\")\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(model_json)\n",
    "\n",
    "# Load the pre-trained model weights\n",
    "model.load_weights(\"facialemotionmodel.h5\")\n",
    "\n",
    "# Load the Haar cascade classifier for face detection\n",
    "haar_file = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "face_cascade = cv2.CascadeClassifier(haar_file)\n",
    "\n",
    "# Define a function to extract features from an image\n",
    "def extract_features(image):\n",
    "    feature = np.array(image)\n",
    "    feature = feature.reshape(1, 48, 48, 1)\n",
    "    return feature / 255.0\n",
    "\n",
    "# Open the webcam (camera)\n",
    "webcam = cv2.VideoCapture(0)\n",
    "\n",
    "# Define labels for emotion classes\n",
    "labels = {0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy', 4: 'neutral', 5: 'sad', 6: 'surprise'}\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the webcam\n",
    "    i, im = webcam.read()\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the grayscale frame\n",
    "    faces = face_cascade.detectMultiScale(im, 1.3, 5)\n",
    "\n",
    "    try:\n",
    "        # For each detected face, perform facial emotion recognition\n",
    "        for (p, q, r, s) in faces:\n",
    "            # Extract the region of interest (ROI) which contains the face\n",
    "            image = gray[q:q + s, p:p + r]\n",
    "\n",
    "            # Draw a rectangle around the detected face\n",
    "            cv2.rectangle(im, (p, q), (p + r, q + s), (255, 0, 0), 2)\n",
    "\n",
    "            # Resize the face image to the required input size (48x48)\n",
    "            image = cv2.resize(image, (48, 48))\n",
    "\n",
    "            # Extract features from the resized face image\n",
    "            img = extract_features(image)\n",
    "\n",
    "            # Make a prediction using the trained model\n",
    "            pred = model.predict(img)\n",
    "\n",
    "            # Get the predicted label for emotion\n",
    "            prediction_label = labels[pred.argmax()]\n",
    "\n",
    "            # Display the predicted emotion label near the detected face\n",
    "            cv2.putText(im, f'Emotion: {prediction_label}', (p - 10, q - 10),\n",
    "                        cv2.FONT_HERSHEY_COMPLEX_SMALL, 2, (0, 0, 255))\n",
    "\n",
    "        # Display the frame with annotations in real-time\n",
    "        cv2.imshow(\"Real-time Facial Emotion Recognition\", im)\n",
    "\n",
    "        # Break the loop if the 'Esc' key is pressed\n",
    "        if cv2.waitKey(1) == 27:\n",
    "            break\n",
    "\n",
    "    except cv2.error:\n",
    "        pass\n",
    "\n",
    "# Release the webcam and close all OpenCV windows\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
